# MLP
MLP implementation from sratch using only numpy and with 1 hidden layer. The activation function used in this case is reLU, with the output layer as a softmax-crossentropy layer. This code only has the training implementation, but the testing can be carried out quite easily by defining a predict function in the cross entropy class and finding the argmax of the output of the softmax function.
